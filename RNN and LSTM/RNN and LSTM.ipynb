{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31708f12",
   "metadata": {
    "id": "31708f12"
   },
   "source": [
    "# Recurrent Neural Networks and Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf12abc",
   "metadata": {
    "id": "adf12abc"
   },
   "source": [
    "## 1. What is meant by Recurrent Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bda9ec-6878-41b0-afeb-7296df82045e",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. \n",
    "This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.\n",
    "This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
    "Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c69c8-39e0-4ece-a383-0e5b73d477be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c2ff04",
   "metadata": {
    "id": "20c2ff04"
   },
   "source": [
    "## 2. What is meant by vanishing and exploding gradient and why is that a problem in RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba163f7-51a5-43c0-94dc-000bc5c4be4c",
   "metadata": {},
   "source": [
    "RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences.\n",
    "The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c3fb6-ecd0-4277-82b2-444bd87f524d",
   "metadata": {},
   "source": [
    "Vanishing â€“\n",
    "As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb15190-7dd1-4e3c-ab10-9eefb90c67fa",
   "metadata": {},
   "source": [
    "The exploding gradient is the inverse of the vanishing gradient and occurs when large error gradients accumulate, resulting in extremely large updates to neural network model weights during training. As a result, the model is unstable and incapable of learning from your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e819185",
   "metadata": {
    "id": "3e819185"
   },
   "source": [
    "## 3. What is meant by Long Short Term Memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cb300-dcbd-41f9-8811-a6396fe7e1fd",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
    "This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d9c0f-2f82-4e8b-bda4-51a5cfa594de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556b916-c930-4f37-9aa6-6041bc90b9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06c8e007",
   "metadata": {
    "id": "06c8e007"
   },
   "source": [
    "## 4. What is meant by Gated Recurrent Unit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb7eda-c089-497d-9023-d0a800956cef",
   "metadata": {},
   "source": [
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. \n",
    "The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. \n",
    "GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.\n",
    "GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b7592-0701-4af2-9bd4-e7ad13774466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99da681",
   "metadata": {
    "id": "d99da681"
   },
   "source": [
    "## 5. Train a bi-directional LSTM on imdb movies sentiment dataset from keras (tutorial available on its website, follow that tutorial) (https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RP7PAJYHNHu9",
   "metadata": {
    "id": "RP7PAJYHNHu9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 20000  \n",
    "maxlen = 200  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4df790-5f09-4612-9c77-8f55ba530f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, train_label), (test_data, test_label) = imdb.load_data()\n",
    "data = np.concatenate((train_data, test_data), axis=0)\n",
    "sentiments = np.concatenate((train_label, test_label), axis=0)\n",
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
    "reviews = []\n",
    "for d in data:\n",
    "    decoded = \" \".join( [inverted_word_index.get(i - 3, \"#\") for i in d] )\n",
    "    reviews.append(decoded)\n",
    "reviews = np.array(reviews)\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bb4a3-7a8f-4220-9f2c-7a465cf09ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(reviews, sentiments, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731b97ba-a899-4617-b500-f2e39ecfba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "X_train = pad_sequences(train_sequences, 500)\n",
    "X_test = pad_sequences(test_sequences, 500)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5246ebfd-885f-4d4f-8981-cd3c9cf0c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 128)        98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,757,761\n",
      "Trainable params: 2,757,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f18dadd5-e95b-42e9-b5a8-fd4511c098f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 50)           4098050   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 256)              183296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,281,603\n",
      "Trainable params: 4,281,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=500))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dda61-6cff-4475-9def-6f27443728c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          train_sentiments,\n",
    "          epochs=3,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, test_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4c745-74cf-494f-b64e-59fa6fae2964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8951d1-6e29-4071-956d-3ac145b1a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, test_sentiments)\n",
    "print(f'Accuracy: {round(accuracy * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW 12 Ishan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
