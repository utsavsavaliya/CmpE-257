{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622f4085",
   "metadata": {},
   "source": [
    "# Dimensionality reduction and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d0a12",
   "metadata": {},
   "source": [
    "## 1. Discuss curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abe8a9-88fc-4401-b79d-d9cffae199ba",
   "metadata": {},
   "source": [
    "--As the number of features increases the efficiency of the algorithm is generally decreases as the number of dimensions increases it requires high computation requirements.\n",
    "--It is also possible that all features are are not equally important for  predictiction in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986bcc5-0456-4cf0-99c4-b9f54d1171cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d7121c",
   "metadata": {},
   "source": [
    "## 2. Discuss any 3 dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3469ef-36dc-4cca-a012-15629455ff89",
   "metadata": {},
   "source": [
    "- Principal component analysis (PCA)\n",
    "- Linear Discriminatory Analysis (LDA)\n",
    "- Singular Value Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f82a2c-63c1-4606-9acf-3c3a0f0fddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13589dcd",
   "metadata": {},
   "source": [
    "## 3. Explain PCA in your own words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67e8b1-ee11-48ec-8b8e-bbca862cba79",
   "metadata": {},
   "source": [
    "PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by \n",
    "projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much\n",
    "of the data's variation as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9b4f0-f496-4fd7-be8e-fb533bc0f322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac07dad8",
   "metadata": {},
   "source": [
    "## 4. Perform classification on wine dataset using any model of your choice using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e1d5ed3-a9e8-4660-b1ec-18e7584ba396",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = datasets.load_wine()\n",
    "X=wine_dataset.data\n",
    "y=wine_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55aa33a-ed45-43ec-8005-92a9366d9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e8d95f7-c01f-45eb-9ca9-b150e51c0285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:59:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[23  0  0]\n",
      " [ 1 22  1]\n",
      " [ 0  0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        23\n",
      "           1       1.00      0.92      0.96        24\n",
      "           2       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.97        63\n",
      "   macro avg       0.97      0.97      0.97        63\n",
      "weighted avg       0.97      0.97      0.97        63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Checkout\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=36)\n",
    "xgb_reg = xgb.XGBClassifier()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = xgb_reg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_predict))\n",
    "print(classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b4e47",
   "metadata": {},
   "source": [
    "## 5. Perform classification on wine dataset using PCA and taking all top components such that 90% variance is explained by them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c557f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "#df= pd.read_csv(wine_dataset, sep=\";\")\n",
    "#df= pd.read_csv(wine_dataset)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97606baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=wine_dataset.data\n",
    "#y=wine_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the dataset\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "151ecc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 8) # creates an instance of PCA class\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4','principal component 5','principal component 6','principal component 7','principal component 8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the final dataframe with 2 principal components and the target variable\n",
    "#finalDf = pd.concat([principalDf, df[['quality']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25509528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1ce3c30-b337-4458-a935-9a0a8dbb7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:04:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[ 9  0  0]\n",
      " [ 2 17  0]\n",
      " [ 0  2 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90         9\n",
      "           1       0.89      0.89      0.89        19\n",
      "           2       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.90      0.93      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Checkout\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# using xgb boost as it is very fast and highly efficient.\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(principalDf,y,test_size=0.25,random_state=48)\n",
    "xgb_reg = xgb.XGBClassifier()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = xgb_reg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_predict))\n",
    "print(classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc258e8-1f7b-4698-830f-4a8092c767f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59cc3aa1",
   "metadata": {},
   "source": [
    "## 6. Discuss the difference in results vs difference in number of features used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55f70c-ce9d-4824-a55f-e9ba905e0fbe",
   "metadata": {},
   "source": [
    "From reducing features from 13(accuracy =0.97) to 8( accuracy =0.91) we reduce the computational reqirements and we even get the similar accuracy only.We also avoided our model from getting overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee0545-3268-42db-9141-7394bc3aa71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
